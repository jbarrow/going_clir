{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset, Field, BucketIterator\n",
    "from vectors import MultiCCA, VectorVocabField\n",
    "from utils import pathify, Checkpoint, load_model\n",
    "from models import SiameseDAN\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.monitor_interval = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = MultiCCA(cache=pathify('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(l, lang = 'en'):\n",
    "    return [lang + ':' + s for s in l]\n",
    "\n",
    "sentence_text = VectorVocabField(lower=True, preprocessing=preprocess)\n",
    "label_field = Field(sequential=False, use_vocab=False, tensor_type=torch.FloatTensor)\n",
    "\n",
    "train, val, test = TabularDataset.splits(\n",
    "    path = pathify('data/sick'), format='tsv', skip_header=True,\n",
    "    train = 'train.txt', test = 'test.txt', validation = 'trial.txt',\n",
    "    fields = [('', None), ('', None), ('s1', sentence_text), ('s2', sentence_text), ('', None), ('score', label_field)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_text.build_vocab(train, vectors=vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "    datasets=(train, test, val), batch_size=32, sort_key = lambda x: len(x.s1), repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loss, iterable, training=True):\n",
    "    batch_accs, batch_losses = [], []\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    for batch in tqdm(iterable, total=len(iterable)):\n",
    "        d, q, y = batch.s1.t(), batch.s2.t(), (batch.score > 3).long()\n",
    "        \n",
    "        if training:\n",
    "            model.zero_grad()\n",
    "\n",
    "        out = model(d, q)\n",
    "        _, preds = torch.max(out, 1)\n",
    "        \n",
    "        accuracy = torch.mean(torch.eq(preds, y).float())\n",
    "        batch_loss = loss(out, y)\n",
    "\n",
    "        if training:\n",
    "            batch_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), .25)\n",
    "            opt.step()\n",
    "\n",
    "        batch_accs.append(accuracy.data[0])\n",
    "        batch_losses.append(batch_loss.data[0])\n",
    "\n",
    "        del d, q, y\n",
    "    \n",
    "    epoch_end = time.time()\n",
    "    return np.mean(batch_accs), np.mean(batch_losses), epoch_end - epoch_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embeddings_dim = sentence_text.vocab.vectors.shape\n",
    "\n",
    "params = {\n",
    "    'vocab_size': vocab_size, \n",
    "    'embedding_dim': embeddings_dim, \n",
    "    'hidden_dim': 100, \n",
    "    'num_classes': 2\n",
    "}\n",
    "\n",
    "clf = SiameseDAN(**params)\n",
    "clf.load_pretrained(sentence_text.vocab.vectors, mode='static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:00<00:00, 354.51it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 426.17it/s]\n",
      "  8%|▊         | 11/139 [00:00<00:01, 105.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5243489593267441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 135.75it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 361.12it/s]\n",
      "  6%|▌         | 8/139 [00:00<00:01, 74.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7454139609615524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 98.64it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 270.66it/s]\n",
      "  6%|▌         | 8/139 [00:00<00:01, 75.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7685470778446692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 93.96it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 366.19it/s]\n",
      "  6%|▌         | 8/139 [00:00<00:01, 78.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7709821427797342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 88.17it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 274.34it/s]\n",
      "  4%|▍         | 6/139 [00:00<00:02, 59.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7895292206244036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 101.70it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 239.23it/s]\n",
      "  7%|▋         | 10/139 [00:00<00:01, 98.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7895292206244036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:00<00:00, 149.03it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 376.95it/s]\n",
      " 10%|█         | 14/139 [00:00<00:00, 139.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7895292206244036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 129.17it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 326.94it/s]\n",
      "  5%|▌         | 7/139 [00:00<00:01, 68.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7945616883890969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 93.42it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 288.56it/s]\n",
      "  6%|▋         | 9/139 [00:00<00:01, 89.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7945616883890969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:00<00:00, 144.26it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 387.98it/s]\n",
      "  5%|▌         | 7/139 [00:00<00:02, 64.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8004464286488372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [00:01<00:00, 91.27it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 335.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8004464286488372\n"
     ]
    }
   ],
   "source": [
    "opt = optim.Adam(filter(lambda p: p.requires_grad, clf.parameters()), lr=5e-2)\n",
    "loss = nn.NLLLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.1)\n",
    "checkpointer = Checkpoint(clf, params, pathify('data/models/sickentest'))\n",
    "\n",
    "save_every = 10\n",
    "\n",
    "init_acc, _, _ = run_epoch(clf, loss, train_iter, training=False)\n",
    "best_acc, _, _ = run_epoch(clf, loss, test_iter, training=False)\n",
    "\n",
    "trn_losses, trn_accs = [0.], [init_acc]\n",
    "val_losses, val_accs = [0.], [best_acc]\n",
    "\n",
    "print(best_acc)\n",
    "\n",
    "for epoch in range(10):\n",
    "    scheduler.step()\n",
    "    \n",
    "    clf.train()\n",
    "    trn_acc, trn_loss, trn_time = run_epoch(clf, loss, train_iter, training=True)\n",
    "    trn_losses.append(trn_loss)\n",
    "    trn_accs.append(trn_acc)\n",
    "        \n",
    "    y_onehot = torch.FloatTensor(32, 2)\n",
    "    clf.eval()\n",
    "    val_acc, val_loss, val_time = run_epoch(clf, loss, val_iter, training=False)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    checkpointer.update(val_acc)\n",
    "    print(checkpointer.best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model = load_model(SiameseDAN, pathify('data/models/sickentest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sentence_text.process(\n",
    "    [['en:the', 'en:man', 'en:went', 'en:for', 'en:a', 'en:jog'], \n",
    "     ['en:he', 'en:went', 'en:jogging']], train=False, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseDAN(\n",
       "  (dan): DAN(\n",
       "    (embedding): Embedding(390271, 512)\n",
       "    (hidden): Linear(in_features=512, out_features=100, bias=True)\n",
       "    (norm_hidden): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (out): Linear(in_features=200, out_features=2, bias=True)\n",
       "  (norm_out): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.7740681)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob, lbl = the_model(a[:, 0].contiguous().view(1, -1), a[:, 1].contiguous().view(1, -1)).max(dim=1)\n",
    "lbl.data[0], np.exp(prob.data.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clir]",
   "language": "python",
   "name": "conda-env-clir-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
